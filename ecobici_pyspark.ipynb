{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, regexp_extract, regexp_replace, broadcast\n",
    "from pyspark.sql.types import StringType, DateType, IntegerType, StructType, StructField\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de Spark\n",
    "spark = SparkSession.builder.appName(\"Ecobici\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carga de los archivos de usuarios y creación de la tabla que se cargará a la BD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_usuario: integer (nullable = true)\n",
      " |-- genero: string (nullable = true)\n",
      " |-- edad: integer (nullable = true)\n",
      " |-- fecha_alta: date (nullable = true)\n",
      "\n",
      "Cantidad de filas: 680092\n",
      "Usuarios nulos\n",
      "+----------+------+----+----------+\n",
      "|id_usuario|genero|edad|fecha_alta|\n",
      "+----------+------+----+----------+\n",
      "+----------+------+----+----------+\n",
      "\n",
      "+----------+------+----+----------+\n",
      "|id_usuario|genero|edad|fecha_alta|\n",
      "+----------+------+----+----------+\n",
      "|        78|     M|  31|2019-02-18|\n",
      "|        85|     M|  30|2019-02-18|\n",
      "|       108|     M|  51|2019-02-18|\n",
      "|       192|     F|  32|2019-02-19|\n",
      "|       211|     M|  39|2019-02-19|\n",
      "|       253|     M|  52|2019-02-19|\n",
      "|       255|     M|  31|2019-02-19|\n",
      "|       300|     M|  50|2019-02-19|\n",
      "|       321|     M|  46|2019-02-20|\n",
      "|       332|     M|  32|2019-02-20|\n",
      "|       463|     M|  22|2019-02-20|\n",
      "|       472|     M|  52|2019-02-20|\n",
      "|       481|     M|  34|2019-02-20|\n",
      "|       497|     M|  28|2019-02-20|\n",
      "|       593|     O|  52|2019-02-20|\n",
      "|       688|     M|  26|2019-02-21|\n",
      "|       723|     M|  63|2019-02-21|\n",
      "|       743|     F|  38|2019-02-21|\n",
      "|       804|     F|  52|2019-02-21|\n",
      "|       833|     M|  46|2019-02-21|\n",
      "+----------+------+----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "usuarios_columnas = [\"id_usuario\", \"genero_usuario\", \"edad_usuario\", \"fecha_alta\"] # Columnas que se van a tener en cuenta\n",
    "carpeta = \"usuarios\" # Carpeta donde estan todos los archivos de usuarios por año\n",
    "dataframes = [] #   Lista donde se van a cargar los dataframes generados para cada archivo\n",
    "\n",
    "for archivo in os.listdir(carpeta): # Iterar sobre la carpeta de usuarios para abrir cada archivo csv que contenga \"usuarios\" en el nombre\n",
    "  if archivo.endswith(\".csv\") and (\"usuarios-ecobici\" in archivo or \"usuarios_ecobici\" in archivo):\n",
    "    ruta_archivo = os.path.join(carpeta, archivo)\n",
    "    df = spark.read.csv(ruta_archivo, header=True, inferSchema=True)\n",
    "\n",
    "    for columna in df.columns:\n",
    "      df = df.withColumnRenamed(columna, columna.lower().replace('\"', '')) # Eliminar las \"\" de las columnas que tienen algunos archivos para dejar todos con el mismo formato\n",
    "\n",
    "    df = df.select(usuarios_columnas) # Quedarse con las columnas seleccionadas\n",
    "    df = df.withColumn(\"fecha_alta\", to_date(col(\"fecha_alta\"), \"dd-MM-yy\")) # Dejar todos los valores de fecha_alta con un mismo formato\n",
    "\n",
    "    dataframes.append(df) # Agregar el dataframe a la lista\n",
    "\n",
    "usuarios_df = dataframes[0]\n",
    "for df in dataframes[1:]:\n",
    "  usuarios_df = usuarios_df.union(df) # Concatenar todos los dataframes\n",
    "\n",
    "usuarios_df = usuarios_df.withColumnRenamed(\"genero_usuario\", \"genero\").withColumnRenamed(\"edad_usuario\", \"edad\") # Renombrar columnas\n",
    "usuarios_df = usuarios_df.select([col(c).alias(c.lower()) for c in usuarios_df.columns]) # Convertir los nombres de las columnas a minúscula\n",
    "usuarios_df = usuarios_df.dropna(subset=[\"edad\"]) # Eliminar las filas donde edad sea Null\n",
    "usuarios_df = usuarios_df.filter(~col(\"edad\").contains(\",\")) # Eliminar las filas donde edad contiene \",\" para poder convertirlo a int\n",
    "usuarios_df = usuarios_df.withColumn(\"genero\", col(\"genero\").substr(1,1)) # Quedarse solo con la primera letra de la columna genero\n",
    "\n",
    "usuarios_df = usuarios_df.withColumn(\"edad\", col(\"edad\").cast(IntegerType())) # Convertir edad a int\n",
    "\n",
    "usuarios_df = usuarios_df.filter((col(\"edad\") >= 10) & (col(\"edad\") <= 125)) # Eliminar las filas con edad inconsistente\n",
    "usuarios_df = usuarios_df.orderBy(\"fecha_alta\", ascending=False) # Ordenar por fecha_alta para eliminar los registros con id_usuario duplicado y mantener los que tengan mayor fecha\n",
    "usuarios_df = usuarios_df.dropDuplicates([\"id_usuario\"]) # Eliminar los duplicados manteniendo el primero (mayor fecha_alta)\n",
    "\n",
    "usuarios_df.printSchema() # Mostrar el esquema del DataFrame\n",
    "print(f\"Cantidad de filas: {usuarios_df.count()}\")\n",
    "nulos_id_usuario = usuarios_df.filter(col(\"id_usuario\").isNull()) # Revisar que no haya id_usuario nulos\n",
    "print(\"Usuarios nulos\")\n",
    "nulos_id_usuario.show()\n",
    "usuarios_df.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carga del archivo de estaciones y creación de la tabla que se cargará a la BD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_estacion: integer (nullable = true)\n",
      " |-- numero_estacion: integer (nullable = true)\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- barrio: string (nullable = true)\n",
      " |-- comuna: integer (nullable = true)\n",
      "\n",
      "Cantidad de filas: 360\n",
      "Estaciones nulas\n",
      "+-----------+---------------+------+------+------+\n",
      "|id_estacion|numero_estacion|nombre|barrio|comuna|\n",
      "+-----------+---------------+------+------+------+\n",
      "+-----------+---------------+------+------+------+\n",
      "\n",
      "+-----------+---------------+-------------------+----------------+------+\n",
      "|id_estacion|numero_estacion|             nombre|          barrio|comuna|\n",
      "+-----------+---------------+-------------------+----------------+------+\n",
      "|          2|              2|           RETIRO I|          RETIRO|     1|\n",
      "|          3|              3|             ADUANA|       MONSERRAT|     1|\n",
      "|          4|              4|         PLAZA ROMA|     SAN NICOLAS|     1|\n",
      "|          5|              5|       PLAZA ITALIA|         PALERMO|    14|\n",
      "|          6|              6|      PARQUE LEZAMA|       SAN TELMO|     1|\n",
      "|          7|              7|           OBELISCO|     SAN NICOLAS|     1|\n",
      "|          8|              8|           CONGRESO|       MONSERRAT|     1|\n",
      "|          9|              9|   PARQUE LAS HERAS|         PALERMO|    14|\n",
      "|         12|             12|PLAZA VICENTE LOPEZ|        RECOLETA|     2|\n",
      "|         13|             13|               ONCE|       BALVANERA|     3|\n",
      "|         14|             14|          PACÍFICO |         PALERMO|    14|\n",
      "|         17|             17|      PLAZA ALMAGRO|         ALMAGRO|     5|\n",
      "|         19|             19|  PLAZA SAN MARTIN |          RETIRO|     1|\n",
      "|         21|             21|   PARQUE PATRICIOS|PARQUE PATRICIOS|     4|\n",
      "|         22|             22|           ARENALES|          RETIRO|     1|\n",
      "|         23|             23|           SUIPACHA|     SAN NICOLAS|     1|\n",
      "|         24|             24|             ALSINA|       MONSERRAT|     1|\n",
      "|         25|             25|       PLAZA GÜEMES|         PALERMO|    14|\n",
      "|         26|             26|      JUANA MANSO I|   PUERTO MADERO|     1|\n",
      "|         27|             27|         MONTEVIDEO|     SAN NICOLAS|     1|\n",
      "+-----------+---------------+-------------------+----------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "estaciones_columnas = [\"ID Comet\", \"NÚMERO de Estación \", \"NOMBRE\", \"BARRIO\", \"COMUNA\"] # Columnas que se van a tener en cuenta\n",
    "\n",
    "estaciones_df = spark.read.csv(\"nuevas-estaciones-bicicletas-publicas.csv\", header=True, sep=\";\", inferSchema=True, encoding=\"ISO-8859-1\", multiLine=True)\n",
    "estaciones_df = estaciones_df.select(estaciones_columnas) # Quedarse con las columnas seleccionadas\n",
    "estaciones_df = estaciones_df.withColumnRenamed(\"ID Comet\", \"id_estacion\").withColumnRenamed(\"NÚMERO de Estación \", \"numero_estacion\") # Renombrar columnas\n",
    "\n",
    "estaciones_df = estaciones_df.select([col(c).alias(c.lower()) for c in estaciones_df.columns]) # Convertir los nombres de las columnas a minúscula\n",
    "estaciones_df = estaciones_df.withColumn(\"comuna\", regexp_extract(\"comuna\", r\"(\\d+)\", 1).cast(IntegerType())) # Quedarse solo con el número de la columna comuna\n",
    "\n",
    "estaciones_df = estaciones_df.withColumn(\"id_estacion\", col(\"id_estacion\").cast(IntegerType())) # Convertir id_estacion a int\n",
    "\n",
    "estaciones_df = estaciones_df.dropDuplicates([\"id_estacion\"]) # Eliminar los duplicados\n",
    "estaciones_df = estaciones_df.dropna(subset=[\"id_estacion\"]) # Eliminar las filas donde id_estacion sea Null\n",
    "\n",
    "estaciones_df.printSchema() # Mostrar el esquema del DataFrame\n",
    "print(f\"Cantidad de filas: {estaciones_df.count()}\")\n",
    "nulos_id_estacion = estaciones_df.filter(col(\"id_estacion\").isNull()) # Revisar que no haya id_estacion nulos\n",
    "print(\"Estaciones nulas\")\n",
    "nulos_id_estacion.show()\n",
    "estaciones_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carga del archivo de viajes y creación de la tabla que se cargará a la BD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valor modelo bicicleta: ICONIC\n",
      "Valor modelo bicicleta: FIT\n",
      "Estaciones nulas\n",
      "+------------+-----------------+------------------+-------------------+----------+----------------+\n",
      "|id_recorrido|duracion_segundos|id_estacion_origen|id_estacion_destino|id_usuario|modelo_bicicleta|\n",
      "+------------+-----------------+------------------+-------------------+----------+----------------+\n",
      "+------------+-----------------+------------------+-------------------+----------+----------------+\n",
      "\n",
      "+------------+-----------------+------------------+-------------------+----------+----------------+\n",
      "|id_recorrido|duracion_segundos|id_estacion_origen|id_estacion_destino|id_usuario|modelo_bicicleta|\n",
      "+------------+-----------------+------------------+-------------------+----------+----------------+\n",
      "|    19632972|             1372|                26|               null|   1020610|             FIT|\n",
      "|    19632583|              466|                38|               null|    774845|             FIT|\n",
      "+------------+-----------------+------------------+-------------------+----------+----------------+\n",
      "\n",
      "+------------+-----------------+------------------+-------------------+----------+----------------+\n",
      "|id_recorrido|duracion_segundos|id_estacion_origen|id_estacion_destino|id_usuario|modelo_bicicleta|\n",
      "+------------+-----------------+------------------+-------------------+----------+----------------+\n",
      "+------------+-----------------+------------------+-------------------+----------+----------------+\n",
      "\n",
      "root\n",
      " |-- id_recorrido: integer (nullable = true)\n",
      " |-- duracion_segundos: integer (nullable = true)\n",
      " |-- id_estacion_origen: integer (nullable = true)\n",
      " |-- id_estacion_destino: integer (nullable = true)\n",
      " |-- id_usuario: integer (nullable = true)\n",
      " |-- modelo_bicicleta: string (nullable = true)\n",
      "\n",
      "Cantidad de filas: 2622329\n",
      "+------------+-----------------+------------------+-------------------+----------+----------------+\n",
      "|id_recorrido|duracion_segundos|id_estacion_origen|id_estacion_destino|id_usuario|modelo_bicicleta|\n",
      "+------------+-----------------+------------------+-------------------+----------+----------------+\n",
      "|    17910696|             1848|               358|                278|    861866|          ICONIC|\n",
      "|    17600256|              288|               444|                  3|    217525|          ICONIC|\n",
      "|    17255670|             1103|               280|                280|    954201|          ICONIC|\n",
      "|    17996972|             1165|               273|                367|    179414|          ICONIC|\n",
      "|    17148836|              378|                65|                 14|      8098|          ICONIC|\n",
      "|    16817654|              747|               477|                477|    940830|          ICONIC|\n",
      "|    17248854|              634|               182|                229|    727902|          ICONIC|\n",
      "|    18036735|              944|                66|                242|    961810|          ICONIC|\n",
      "|    17993884|             1353|               132|                 75|    979125|          ICONIC|\n",
      "|    16944859|             1565|                91|                254|     93862|          ICONIC|\n",
      "|    17206438|              947|               465|                383|    352850|          ICONIC|\n",
      "|    17443342|             1068|               128|                 73|    344641|          ICONIC|\n",
      "|    17448732|             1143|                41|                425|    962201|          ICONIC|\n",
      "|    17163317|             1808|                99|                 44|    469918|          ICONIC|\n",
      "|    18014536|             1265|                96|                104|    432079|          ICONIC|\n",
      "|    17099263|              341|                70|                212|    814540|          ICONIC|\n",
      "|    17911060|              750|               359|                416|    949365|          ICONIC|\n",
      "|    18034893|             1468|                91|                448|    806375|          ICONIC|\n",
      "|    18018970|             1108|               204|                131|    828574|          ICONIC|\n",
      "|    18896462|              852|               190|                212|   1008482|          ICONIC|\n",
      "+------------+-----------------+------------------+-------------------+----------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "viajes_columnas = [\"Id_recorrido\",\"duracion_recorrido\",\"id_estacion_origen\",\"id_estacion_destino\",\"id_usuario\",\"modelo_bicicleta\"] # Columnas que se van a tener en cuenta\n",
    "\n",
    "viajes_df = spark.read.csv(\"trips_2023.csv\", header=True, inferSchema=True)\n",
    "viajes_df = viajes_df.select(viajes_columnas) # Quedarse con las columnas seleccionadas\n",
    "viajes_df = viajes_df.withColumnRenamed(\"duracion_recorrido\", \"duracion_segundos\") # Renombrar columnas\n",
    "viajes_df = viajes_df.select([col(c).alias(c.lower()) for c in viajes_df.columns]) # Convertir los nombres de las columnas a minúscula\n",
    "\n",
    "columnas_baecobici = [\"id_recorrido\", \"id_estacion_origen\", \"id_estacion_destino\", \"id_usuario\"]\n",
    "for columna in columnas_baecobici:\n",
    "      viajes_df = viajes_df.withColumn(columna, regexp_extract(columna, r'(\\d+)', 1)) # Eliminar \"BAEcobici\" de las columnas que lo tienen\n",
    "\n",
    "valores_modelo = viajes_df.select(\"modelo_bicicleta\").distinct().collect() # Obtener valores únicos de la columna modelo_bicicleta para saber como almacenarlo en la BD\n",
    "for valor in valores_modelo:\n",
    "    print(f\"Valor modelo bicicleta: {valor[0]}\") # Mostrar los valores únicos\n",
    "\n",
    "viajes_df = viajes_df.withColumn(\"duracion_segundos\", regexp_replace(col(\"duracion_segundos\"), \",\", \"\")) # Quitar las comas de la columna duracion_segundos\n",
    "\n",
    "columnas_int = [\"id_recorrido\",\"duracion_segundos\",\"id_estacion_origen\",\"id_estacion_destino\",\"id_usuario\"]\n",
    "for columna in columnas_int:\n",
    "  viajes_df = viajes_df.withColumn(columna, col(columna).cast(IntegerType())) # Convertir columnas a int\n",
    "\n",
    "nulos_estacion_origen = viajes_df.filter(col(\"id_estacion_origen\").isNull()) # Revisar que no haya id_estacion_origen nulos\n",
    "nulos_estacion_destino = viajes_df.filter(col(\"id_estacion_destino\").isNull()) # Revisar que no haya id_estacion_destinon nulos\n",
    "nulos_id_recorrido = viajes_df.filter(col(\"id_recorrido\").isNull()) # Revisar que no haya id_registro nulos\n",
    "print(\"Estaciones nulas\")\n",
    "nulos_estacion_origen.show()\n",
    "nulos_estacion_destino.show()\n",
    "nulos_id_recorrido.show()\n",
    "viajes_df = viajes_df.dropna() # Eliminar filas nulas\n",
    "viajes_df.printSchema() # Mostrar el esquema del DataFrame\n",
    "print(f\"Cantidad de filas: {viajes_df.count()}\")\n",
    "viajes_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtrar el dataframe de viajes para solo conservar los registros donde id_usuario exista en el dataframe usuarios y id_estacion origen y id_estacion_destion existan en el dataframe estaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_recorrido: integer (nullable = true)\n",
      " |-- duracion_segundos: integer (nullable = true)\n",
      " |-- id_estacion_origen: integer (nullable = true)\n",
      " |-- id_estacion_destino: integer (nullable = true)\n",
      " |-- id_usuario: integer (nullable = true)\n",
      " |-- modelo_bicicleta: string (nullable = true)\n",
      "\n",
      "Cantidad de filas: 2456749\n",
      "+------------+-----------------+------------------+-------------------+----------+----------------+\n",
      "|id_recorrido|duracion_segundos|id_estacion_origen|id_estacion_destino|id_usuario|modelo_bicicleta|\n",
      "+------------+-----------------+------------------+-------------------+----------+----------------+\n",
      "|    17910696|             1848|               358|                278|    861866|          ICONIC|\n",
      "|    17600256|              288|               444|                  3|    217525|          ICONIC|\n",
      "|    17255670|             1103|               280|                280|    954201|          ICONIC|\n",
      "|    17996972|             1165|               273|                367|    179414|          ICONIC|\n",
      "|    17148836|              378|                65|                 14|      8098|          ICONIC|\n",
      "|    16817654|              747|               477|                477|    940830|          ICONIC|\n",
      "|    17248854|              634|               182|                229|    727902|          ICONIC|\n",
      "|    18036735|              944|                66|                242|    961810|          ICONIC|\n",
      "|    17993884|             1353|               132|                 75|    979125|          ICONIC|\n",
      "|    17206438|              947|               465|                383|    352850|          ICONIC|\n",
      "|    17443342|             1068|               128|                 73|    344641|          ICONIC|\n",
      "|    17448732|             1143|                41|                425|    962201|          ICONIC|\n",
      "|    17163317|             1808|                99|                 44|    469918|          ICONIC|\n",
      "|    18014536|             1265|                96|                104|    432079|          ICONIC|\n",
      "|    17099263|              341|                70|                212|    814540|          ICONIC|\n",
      "|    17911060|              750|               359|                416|    949365|          ICONIC|\n",
      "|    18034893|             1468|                91|                448|    806375|          ICONIC|\n",
      "|    18018970|             1108|               204|                131|    828574|          ICONIC|\n",
      "|    18896462|              852|               190|                212|   1008482|          ICONIC|\n",
      "|    17205760|             1065|               166|                156|    934038|          ICONIC|\n",
      "+------------+-----------------+------------------+-------------------+----------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Alias para los DataFrames\n",
    "usuarios_alias = usuarios_df.alias(\"u\")\n",
    "estaciones_alias = estaciones_df.alias(\"e\")\n",
    "viajes_alias = viajes_df.alias(\"v\")\n",
    "\n",
    "# Unir usuarios_df y estaciones_df con viajes_df\n",
    "viajes_filtrado = viajes_alias.join(usuarios_alias, on=\"id_usuario\", how=\"inner\") \\\n",
    "    .join(estaciones_alias, col(\"v.id_estacion_origen\") == col(\"e.id_estacion\"), how=\"inner\") \\\n",
    "    .join(estaciones_alias.alias(\"e_destino\"), col(\"v.id_estacion_destino\") == col(\"e_destino.id_estacion\"), how=\"inner\") \\\n",
    "    .select(\"v.id_recorrido\", \"v.duracion_segundos\", \"v.id_estacion_origen\", \"v.id_estacion_destino\", \"v.id_usuario\", \"v.modelo_bicicleta\")\n",
    "\n",
    "viajes_filtrado.printSchema() # Mostrar el esquema del DataFrame\n",
    "# Mostrar el DataFrame resultante\n",
    "print(f\"Cantidad de filas: {viajes_filtrado.count()}\")\n",
    "viajes_filtrado.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer el archivo de configuración\n",
    "with open(\"config.json\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Acceder a las configuraciones\n",
    "user = config[\"DB_USER\"]\n",
    "password = config[\"DB_PASSWORD\"]\n",
    "host = config[\"DB_HOST\"]\n",
    "database = config[\"DB_DATABASE\"]\n",
    "port = config[\"DB_PORT\"]\n",
    "\n",
    "driver = \"org.postgresql.Driver\"\n",
    "\n",
    "# Configurar las credenciales de PostgreSQL\n",
    "postgresql_url = f\"jdbc:postgresql://{host}:{port}/{database}\"\n",
    "properties = {\"user\": user, \"password\": password, \"driver\": \"org.postgresql.Driver\"}\n",
    "\n",
    "# Esquema de las tablas\n",
    "usuarios_schema = \"id_usuario INT PRIMARY KEY, genero CHAR(1), edad SMALLINT, fecha_alta DATE\"\n",
    "estaciones_schema = \"id_estacion INT PRIMARY KEY, numero_estacion INT, nombre VARCHAR(70), barrio VARCHAR(50), comuna SMALLINT\"\n",
    "viajes_schema = \"id_recorrido INT PRIMARY KEY, duracion_segundos INT, id_estacion_origen INT REFERENCES estaciones(id_estacion), id_estacion_destino INT REFERENCES estaciones(id_estacion), id_usuario INT REFERENCES usuarios(id_usuario), modelo_bicicleta VARCHAR(10)\"\n",
    "\n",
    "\n",
    "# Obtener la conexión JDBC\n",
    "connection = spark._jvm.java.sql.DriverManager.getConnection(postgresql_url, properties[\"user\"], properties[\"password\"])\n",
    "# Crear un objeto Statement\n",
    "statement = connection.createStatement()\n",
    "# Sentencia SQL para crear la tabla si no existe\n",
    "tablas = [\"viajes\", \"usuarios\", \"estaciones\"]\n",
    "for tabla in tablas:\n",
    "        statement.execute(f\"DROP TABLE IF EXISTS {tabla}\")\n",
    "\n",
    "statement.execute(f\"CREATE TABLE usuarios ({usuarios_schema})\")\n",
    "statement.execute(f\"CREATE TABLE estaciones ({estaciones_schema})\")\n",
    "statement.execute(f\"CREATE TABLE viajes ({viajes_schema})\")\n",
    "\n",
    "statement.execute(\"CREATE INDEX idx_id_usuario ON viajes (id_usuario)\")\n",
    "statement.execute(\"CREATE INDEX idx_id_estacion_origen ON viajes (id_estacion_origen)\")\n",
    "statement.execute(\"CREATE INDEX idx_id_estacion_destino ON viajes (id_estacion_destino)\")\n",
    "\n",
    "# Escribir el DataFrame en la tabla PostgreSQL\n",
    "usuarios_df.write.jdbc(url=postgresql_url, table=\"usuarios\", mode=\"append\", properties=properties)\n",
    "estaciones_df.write.jdbc(url=postgresql_url, table=\"estaciones\", mode=\"append\", properties=properties)\n",
    "viajes_filtrado.write.jdbc(url=postgresql_url, table=\"viajes\", mode=\"append\", properties=properties)\n",
    "\n",
    "# Cerrar SparkSession\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
